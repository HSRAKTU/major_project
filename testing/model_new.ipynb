{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218ba7f2",
   "metadata": {},
   "source": [
    "# Importing Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fcd0faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Imports and Seed Setup\n",
    "# -----------------------------\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib  # for saving the scaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  \n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e54c9f",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951a8a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Average Cd",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "953af21b-bab3-4893-8a61-29efdb6f529d",
       "rows": [
        [
         "count",
         "7713.0"
        ],
        [
         "mean",
         "0.2844122596453961"
        ],
        [
         "std",
         "0.03723220287049474"
        ],
        [
         "min",
         "0.201138367156146"
        ],
        [
         "25%",
         "0.255859366461794"
        ],
        [
         "50%",
         "0.282986553124688"
        ],
        [
         "75%",
         "0.311521421763092"
        ],
        [
         "max",
         "0.383329962159601"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average Cd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7713.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.284412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.037232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.201138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.255859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.282987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.311521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.383330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Average Cd\n",
       "count  7713.000000\n",
       "mean      0.284412\n",
       "std       0.037232\n",
       "min       0.201138\n",
       "25%       0.255859\n",
       "50%       0.282987\n",
       "75%       0.311521\n",
       "max       0.383330"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('..\\data\\DrivAerNetPlusPlus_Drag_8k_cleaned.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c14b960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Using device: cuda\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Memory Allocated: 0.0 MB\n",
      "Memory Cached: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Get current device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Print GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Memory Allocated:\", torch.cuda.memory_allocated(0) / 1024**2, \"MB\")\n",
    "    print(\"Memory Cached:\", torch.cuda.memory_reserved(0) / 1024**2, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7fe6b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Average Cd', ylabel='Count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the pdf graph for Average Drag Coefficient (Cd)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Average Cd'], bins=30, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41516c9c",
   "metadata": {},
   "source": [
    "# Pepraring Scaler Function and Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a83241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler mean: 0.284506, std: 0.037448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../outputs/cd_scaler.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1.2. Prepaing Scaler\n",
    "# -----------------------------\n",
    "\n",
    "# Subset to only training car IDs\n",
    "with open(\"../data/subset_dir/train_design_ids.txt\") as f:\n",
    "    train_ids = [line.strip() for line in f]\n",
    "\n",
    "df_train = df[df[\"Design\"].isin(train_ids)]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train[[\"Average Cd\"]])\n",
    "\n",
    "print(f\"Scaler mean: {scaler.mean_[0]:.6f}, std: {scaler.scale_[0]:.6f}\")\n",
    "\n",
    "# Save scaler to disk\n",
    "os.makedirs(\"../outputs\", exist_ok=True)\n",
    "joblib.dump(scaler, \"../outputs/cd_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffb8f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1.3. Global Variables\n",
    "# -----------------------------\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# point_net_input_dim = 2\n",
    "# point_net_embedding_dim = 512\n",
    "# lstm_input_dim = point_net_embedding_dim\n",
    "# lstm_hidden_dim = 512\n",
    "# bidirectional = True\n",
    "# number_of_layers = 2\n",
    "# cd_regressor_input_dim = lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim\n",
    "# batch_size = 2\n",
    "# checkpoint_dir=\"../outputs/checkpoints3\"\n",
    "# analysis_output_dir='../outputs/analysis'\n",
    "# scaler = joblib.load(\"../outputs/cd_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb0d7618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1.3. Global Variables\n",
    "# -----------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "point_net_input_dim = 2\n",
    "point_net_embedding_dim = 256\n",
    "lstm_input_dim = point_net_embedding_dim\n",
    "lstm_hidden_dim = 256\n",
    "bidirectional = True\n",
    "number_of_layers = 2\n",
    "cd_regressor_input_dim = lstm_hidden_dim * 2 if bidirectional else lstm_hidden_dim\n",
    "batch_size = 4\n",
    "\n",
    "model_checkpoint_dir=\"../outputs/checkpoints3\"\n",
    "analysis_output_dir='../outputs/analysis'\n",
    "\n",
    "scaler = joblib.load(\"../outputs/cd_scaler.pkl\")\n",
    "assert hasattr(scaler, \"mean_\"), \"Scaler not loaded correctly\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da915d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20024180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # 2. Enhanced PointNet2D (Optimized)\n",
    "# # -----------------------------\n",
    "# class PointNet2D(nn.Module):\n",
    "#     def __init__(self, input_dim=2, emb_dim=512):\n",
    "#         super(PointNet2D, self).__init__()\n",
    "\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Conv1d(input_dim, 64, 1),\n",
    "#             nn.BatchNorm1d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(64, 128, 1),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(128, 256, 1),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(256, emb_dim, 1),\n",
    "#             nn.BatchNorm1d(emb_dim),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         # Ensure input and model are on the same device\n",
    "#         assert x.device == next(self.parameters()).device, \"Input and model are on different devices!\"\n",
    "\n",
    "#         # x: (B, N, 2) -> (B, 2, N)\n",
    "#         x = x.transpose(1, 2)\n",
    "#         features = self.mlp(x)  # (B, emb_dim, N)\n",
    "\n",
    "#         if mask is not None:\n",
    "#             mask = mask.unsqueeze(1)  # (B, 1, N)\n",
    "#             features = features.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "#         embedding = torch.max(features, dim=2)[0]  # (B, emb_dim)\n",
    "#         return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e42f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# LightPointNet2D: Efficient 2D PointNet\n",
    "# -----------------------------\n",
    "class PointNet2D(nn.Module):\n",
    "    def __init__(self, input_dim=2, emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 32, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(32, 64, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(64, emb_dim, 1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (B*N, P, 2) → (B*N, 2, P)\n",
    "        x = x.transpose(1, 2)  # (B, 2, N)\n",
    "        features = self.mlp(x)  # (B, emb_dim, N)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # (B, 1, N)\n",
    "            features = features.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Global max pooling\n",
    "        embedding = torch.max(features, dim=2)[0]  # (B, emb_dim)\n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0225096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. LSTM Slice Encoder (Optimized)\n",
    "# -----------------------------\n",
    "class LSTMSliceEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=512, num_layers=2, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=0.2  # Dropout between layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input and model are on the same device\n",
    "        assert x.device == next(self.parameters()).device, \"Input and model are on different devices!\"\n",
    "\n",
    "        # x: (B, S, D)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        if self.lstm.bidirectional:\n",
    "            return torch.cat((h_n[-2], h_n[-1]), dim=-1)  # (B, 2H)\n",
    "        else:\n",
    "            return h_n[-1]  # (B, H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "824a7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Cd Regressor MLP (Optimized)\n",
    "# -----------------------------\n",
    "class CdRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=1024):  # 512 * 2 from BiLSTM\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input and model are on the same device\n",
    "        assert x.device == next(self.parameters()).device, \"Input and model are on different devices!\"\n",
    "        return self.net(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3870d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Full Model Assembly (Optimized)\n",
    "# -----------------------------\n",
    "class CdPredictorNet(nn.Module):\n",
    "    def __init__(self, pointnet, lstm_encoder, regressor):\n",
    "        super().__init__()\n",
    "        self.pointnet = pointnet\n",
    "        self.lstm_encoder = lstm_encoder\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def forward(self, slices, point_mask, slice_mask):\n",
    "        # slices: (B, S, N, D)\n",
    "        B, S, N, D = slices.shape\n",
    "\n",
    "        # Optional: Device consistency check\n",
    "        assert slices.device == next(self.parameters()).device, \"Input slices not on same device as model.\"\n",
    "\n",
    "        # Flatten slices and masks\n",
    "        flat_slices = slices.reshape(B * S, N, D)\n",
    "        flat_mask = point_mask.reshape(B * S, N)\n",
    "\n",
    "        # Encode each slice\n",
    "        slice_embs = self.pointnet(flat_slices, flat_mask)  # (B*S, 256)\n",
    "        slice_embs = slice_embs.view(B, S, -1)  # (B, S, 256)\n",
    "\n",
    "        # Temporal encoding with LSTM\n",
    "        car_emb = self.lstm_encoder(slice_embs)  # (B, 1024 if bidirectional)\n",
    "\n",
    "        # Final regression\n",
    "        return self.regressor(car_emb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6fcc65",
   "metadata": {},
   "source": [
    "# Dataset loader and Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "385df74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Dataset Loader (Optimized)\n",
    "# -----------------------------\n",
    "class CarSlicesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids_txt, npz_dir, csv_path, max_cars=None, scaler = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ids_txt (str): Path to the text file containing car IDs.\n",
    "            npz_dir (str): Directory containing the .npz files.\n",
    "            csv_path (str): Path to the CSV file with Cd values.\n",
    "            max_cars (int, optional): Limit the number of cars to load. Defaults to None.\n",
    "            scaler (object, optional): Scaler object for normalizing Cd values. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.car_ids = [line.strip() for line in open(ids_txt)]\n",
    "        if max_cars:\n",
    "            self.car_ids = self.car_ids[:max_cars]\n",
    "        self.npz_dir = npz_dir\n",
    "        self.cd_map = pd.read_csv(csv_path).set_index(\"Design\")[\"Average Cd\"].to_dict()\n",
    "        self.scaler = scaler if scaler else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.car_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        car_id = self.car_ids[idx]\n",
    "        data = np.load(os.path.join(self.npz_dir, f\"{car_id}_axis-x.npz\"))\n",
    "\n",
    "        # Keep in NumPy for now (better for pin_memory and batch collation)\n",
    "        slices = data[\"slices\"].astype(np.float32)         # (80, 6500, 2)\n",
    "        point_mask = data[\"point_mask\"].astype(np.float32) # (80, 6500)\n",
    "        slice_mask = data[\"slice_mask\"].astype(np.float32) # (80,)\n",
    "        cd_value = np.float32(self.cd_map[car_id])\n",
    "\n",
    "        if self.scaler:\n",
    "            cd_value = self.scaler.transform([[cd_value]])[0, 0]\n",
    "\n",
    "        return slices, point_mask, slice_mask, cd_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80ec6e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CdPredictorNet                           [2]                       --\n",
       "├─PointNet2D: 1-1                        [160, 256]                --\n",
       "│    └─Sequential: 2-1                   [160, 256, 6500]          --\n",
       "│    │    └─Conv1d: 3-1                  [160, 32, 6500]           96\n",
       "│    │    └─ReLU: 3-2                    [160, 32, 6500]           --\n",
       "│    │    └─Conv1d: 3-3                  [160, 64, 6500]           2,112\n",
       "│    │    └─ReLU: 3-4                    [160, 64, 6500]           --\n",
       "│    │    └─Conv1d: 3-5                  [160, 256, 6500]          16,640\n",
       "│    │    └─ReLU: 3-6                    [160, 256, 6500]          --\n",
       "├─LSTMSliceEncoder: 1-2                  [2, 512]                  --\n",
       "│    └─LSTM: 2-2                         [2, 80, 512]              2,629,632\n",
       "├─CdRegressor: 1-3                       [2]                       --\n",
       "│    └─Sequential: 2-3                   [2, 1]                    --\n",
       "│    │    └─Linear: 3-7                  [2, 256]                  131,328\n",
       "│    │    └─ReLU: 3-8                    [2, 256]                  --\n",
       "│    │    └─Dropout: 3-9                 [2, 256]                  --\n",
       "│    │    └─Linear: 3-10                 [2, 64]                   16,448\n",
       "│    │    └─ReLU: 3-11                   [2, 64]                   --\n",
       "│    │    └─Linear: 3-12                 [2, 1]                    65\n",
       "==========================================================================================\n",
       "Total params: 2,796,321\n",
       "Trainable params: 2,796,321\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 20.02\n",
       "==========================================================================================\n",
       "Input size (MB): 12.48\n",
       "Forward/backward pass size (MB): 2929.30\n",
       "Params size (MB): 11.19\n",
       "Estimated Total Size (MB): 2952.97\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model\n",
    "pointnet = PointNet2D(input_dim=point_net_input_dim, emb_dim=point_net_embedding_dim)\n",
    "lstm_encoder = LSTMSliceEncoder(input_dim=lstm_input_dim, hidden_dim=lstm_hidden_dim, num_layers=number_of_layers, bidirectional=bidirectional)\n",
    "regressor = CdRegressor(input_dim=cd_regressor_input_dim)\n",
    "model = CdPredictorNet(pointnet, lstm_encoder, regressor).to(device)\n",
    "\n",
    "# Print model summary (example input: batch_size=2, slices=80, points=6500, dim=2)\n",
    "summary(model, input_data=(\n",
    "    torch.zeros(2, 80, 6500, 2).to(device),\n",
    "    torch.ones(2, 80, 6500).to(device),\n",
    "    torch.ones(2, 80).to(device)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75f8f7",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0a613",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e9c6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, val_loader, device, scaler):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for slices, point_mask, slice_mask, cd_gt in val_loader:\n",
    "            slices = slices.to(device, non_blocking=True)\n",
    "            point_mask = point_mask.to(device, non_blocking=True)\n",
    "            slice_mask = slice_mask.to(device, non_blocking=True)\n",
    "            cd_gt = cd_gt.to(device, non_blocking=True).float()\n",
    "\n",
    "            pred = model(slices, point_mask, slice_mask)\n",
    "            preds.append(pred)\n",
    "            trues.append(cd_gt)\n",
    "\n",
    "    preds = torch.cat(preds).cpu().numpy()\n",
    "    trues = torch.cat(trues).cpu().numpy()\n",
    "\n",
    "    if scaler:\n",
    "        preds = scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "        trues = scaler.inverse_transform(trues.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Metrics\n",
    "    r2 = r2_score(trues, preds)\n",
    "    mse = mean_squared_error(trues, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(trues, preds)\n",
    "    mape = np.mean(np.abs((trues - preds) / (trues + 1e-8))) * 100  # small constant for stability\n",
    "\n",
    "    # Normalized Absolute Error (based on range)\n",
    "    value_range = np.max(trues) - np.min(trues)\n",
    "    nmae = np.mean(np.abs(trues - preds) / (value_range + 1e-8))\n",
    "\n",
    "    print(f\"\\n📊 Validation Metrics:\")\n",
    "    print(f\"🔹 R²   = {r2:.4f}\")\n",
    "    print(f\"🔹 MSE  = {mse:.6f}\")\n",
    "    print(f\"🔹 RMSE = {rmse:.6f}\")\n",
    "    print(f\"🔹 MAE  = {mae:.6f}\")\n",
    "    print(f\"🔹 MAPE = {mape:.2f}%\")\n",
    "    print(f\"🔹 NMAE  = {nmae:.4f}\")\n",
    "    print(f\"⏱️ Time taken: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "    return {\"r2\": r2, \"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"mape\": mape, \"nmae\": nmae}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92019c9a",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed7b25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. Training Loop (Updated)\n",
    "# -----------------------------\n",
    "\n",
    "def train_model(resume=True, num_epochs=50, max_cars=2000, validation_set_size=200,\n",
    "                checkpoint_dir=model_checkpoint_dir, early_stopping_patience=5):\n",
    "    import time\n",
    "\n",
    "    # Initialize model using global config\n",
    "    pointnet = PointNet2D(input_dim=point_net_input_dim, emb_dim=point_net_embedding_dim)\n",
    "    lstm_encoder = LSTMSliceEncoder(\n",
    "        input_dim=lstm_input_dim,\n",
    "        hidden_dim=lstm_hidden_dim,\n",
    "        num_layers=number_of_layers,\n",
    "        bidirectional=bidirectional\n",
    "    )\n",
    "    regressor = CdRegressor(input_dim=cd_regressor_input_dim)\n",
    "    model = CdPredictorNet(pointnet, lstm_encoder, regressor).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    start_epoch = 1\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Resume from checkpoint if available\n",
    "    if resume:\n",
    "        checkpoints = sorted(glob.glob(os.path.join(checkpoint_dir, \"epoch_*_loss*.pt\")))\n",
    "        if checkpoints:\n",
    "            latest_ckpt = checkpoints[-1]\n",
    "            print(f\"🔄 Resuming from checkpoint: {latest_ckpt}\")\n",
    "            state = torch.load(latest_ckpt, map_location=device)\n",
    "            model.load_state_dict(state['model'])\n",
    "            optimizer.load_state_dict(state['optimizer'])\n",
    "            start_epoch = state['epoch'] + 1\n",
    "            epoch_losses = state.get('epoch_losses', [])\n",
    "            best_loss = min(epoch_losses) if epoch_losses else float(\"inf\")\n",
    "        else:\n",
    "            print(\"⏩ No previous checkpoint found, starting fresh.\")\n",
    "\n",
    "    # Dataset & Dataloaders\n",
    "    train_dataset = CarSlicesDataset(\n",
    "        ids_txt=\"../data/subset_dir/train_design_ids.txt\",\n",
    "        npz_dir=\"../outputs/pad_masked_slices\",\n",
    "        csv_path=\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\",\n",
    "        max_cars=max_cars,\n",
    "        scaler=scaler\n",
    "    )\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_dataset = CarSlicesDataset(\n",
    "        ids_txt=\"../data/subset_dir/val_design_ids.txt\",\n",
    "        npz_dir=\"../outputs/pad_masked_slices\",\n",
    "        csv_path=\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\",\n",
    "        max_cars=validation_set_size,\n",
    "        scaler=scaler\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    print(f\"📦 Training on {len(train_dataset)} samples, validating on {len(val_dataset)} samples.\")\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, num_epochs + 1):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            start_time = time.time()\n",
    "            pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch}\", unit=\"batch\")\n",
    "\n",
    "            for slices, point_mask, slice_mask, cd_gt in pbar:\n",
    "\n",
    "                slices = slices.to(device, non_blocking=True)\n",
    "                point_mask = point_mask.to(device, non_blocking=True)\n",
    "                slice_mask = slice_mask.to(device, non_blocking=True)\n",
    "                cd_gt = cd_gt.to(device, non_blocking=True).float()\n",
    "\n",
    "                pred = model(slices, point_mask, slice_mask)\n",
    "                loss = loss_fn(pred, cd_gt)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_loss = loss.item() * slices.size(0)\n",
    "                total_loss += batch_loss\n",
    "                pbar.set_postfix(loss=batch_loss / slices.size(0))\n",
    "\n",
    "            avg_loss = total_loss / len(train_dataloader.dataset)\n",
    "            epoch_losses.append(avg_loss)\n",
    "            print(f\"\\n✅ Epoch {epoch}: Avg SmoothL1 Loss = {avg_loss:.7f}\")\n",
    "\n",
    "            # 🔍 Validation\n",
    "            print(f\"🔍 Evaluating on validation set:\")\n",
    "            evaluate_model(model, val_loader, device, scaler)\n",
    "\n",
    "            print(f\"⏱️ Epoch Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "            # 💾 Save checkpoint\n",
    "            ckpt_path = os.path.join(checkpoint_dir, f\"epoch_{epoch:02d}_loss_{avg_loss:.8f}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch_losses': epoch_losses\n",
    "            }, ckpt_path)\n",
    "            print(f\"💾 Checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "            # 🏆 Save best model separately\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"best_model.pt\"))\n",
    "                print(\"🏆 Best model updated.\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(f\"🛑 Early stopping at epoch {epoch}.\")\n",
    "                    break\n",
    "\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⛔ Interrupted. Saving checkpoint...\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch_losses': epoch_losses\n",
    "        }, os.path.join(checkpoint_dir, f\"interrupted_epoch_{epoch}.pt\"))\n",
    "        print(\"🧷 Last checkpoint saved.\")\n",
    "\n",
    "    return model, epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22f81c",
   "metadata": {},
   "source": [
    "## Debugging Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e211dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # 7. Training Loop (Degubbing)\n",
    "# # -----------------------------\n",
    "\n",
    "# def train_model(resume=True, num_epochs=50, max_cars=2000,\n",
    "#                 checkpoint_dir=model_checkpoint_dir, early_stopping_patience=5):\n",
    "#     import time\n",
    "\n",
    "#     # Initialize model using global config\n",
    "#     pointnet = PointNet2D(input_dim=point_net_input_dim, emb_dim=point_net_embedding_dim)\n",
    "#     lstm_encoder = LSTMSliceEncoder(\n",
    "#         input_dim=lstm_input_dim,\n",
    "#         hidden_dim=lstm_hidden_dim,\n",
    "#         num_layers=number_of_layers,\n",
    "#         bidirectional=bidirectional\n",
    "#     )\n",
    "#     regressor = CdRegressor(input_dim=cd_regressor_input_dim)\n",
    "#     model = CdPredictorNet(pointnet, lstm_encoder, regressor).to(device)\n",
    "\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "#     loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "#     start_epoch = 1\n",
    "#     best_loss = float(\"inf\")\n",
    "#     patience_counter = 0\n",
    "#     epoch_losses = []\n",
    "\n",
    "#     # Resume from checkpoint if available\n",
    "#     if resume:\n",
    "#         checkpoints = sorted(glob.glob(os.path.join(checkpoint_dir, \"epoch_*_loss*.pt\")))\n",
    "#         if checkpoints:\n",
    "#             latest_ckpt = checkpoints[-1]\n",
    "#             print(f\"🔄 Resuming from checkpoint: {latest_ckpt}\")\n",
    "#             state = torch.load(latest_ckpt, map_location=device)\n",
    "#             model.load_state_dict(state['model'])\n",
    "#             optimizer.load_state_dict(state['optimizer'])\n",
    "#             start_epoch = state['epoch'] + 1\n",
    "#             epoch_losses = state.get('epoch_losses', [])\n",
    "#             best_loss = min(epoch_losses) if epoch_losses else float(\"inf\")\n",
    "#         else:\n",
    "#             print(\"⏩ No previous checkpoint found, starting fresh.\")\n",
    "\n",
    "#     # Dataset & Dataloaders\n",
    "#     train_dataset = CarSlicesDataset(\n",
    "#         ids_txt=\"../data/subset_dir/train_design_ids.txt\",\n",
    "#         npz_dir=\"../outputs/pad_masked_slices\",\n",
    "#         csv_path=\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\",\n",
    "#         max_cars=max_cars,\n",
    "#         scaler=scaler\n",
    "#     )\n",
    "#     train_dataloader = torch.utils.data.DataLoader(\n",
    "#         train_dataset, batch_size=batch_size, shuffle=True,\n",
    "#         num_workers = 0,\n",
    "#         pin_memory = False,\n",
    "#         persistent_workers = False\n",
    "#     )\n",
    "\n",
    "#     val_dataset = CarSlicesDataset(\n",
    "#         ids_txt=\"../data/subset_dir/val_design_ids.txt\",\n",
    "#         npz_dir=\"../outputs/pad_masked_slices\",\n",
    "#         csv_path=\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\",\n",
    "#         scaler=scaler\n",
    "#     )\n",
    "#     val_loader = torch.utils.data.DataLoader(\n",
    "#         val_dataset,\n",
    "#         batch_size=4,\n",
    "#         shuffle=False,\n",
    "#         num_workers = 4,\n",
    "#         pin_memory = False,\n",
    "#         persistent_workers = False\n",
    "#     )\n",
    "\n",
    "#     print(f\"📦 Training on {len(train_dataset)} samples, validating on {len(val_dataset)} samples.\")\n",
    "\n",
    "#     try:\n",
    "#         for epoch in range(start_epoch, num_epochs + 1):\n",
    "#             model.train()\n",
    "#             total_loss = 0.0\n",
    "#             start_time = time.time()\n",
    "#             pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch}\", unit=\"batch\")\n",
    "\n",
    "#             for slices, point_mask, slice_mask, cd_gt in pbar:\n",
    "                \n",
    "#                 start = time.time()\n",
    "                \n",
    "#                 slices = slices.to(device, non_blocking=True)\n",
    "#                 point_mask = point_mask.to(device, non_blocking=True)\n",
    "#                 slice_mask = slice_mask.to(device, non_blocking=True)\n",
    "#                 cd_gt = cd_gt.to(device, non_blocking=True).float()\n",
    "\n",
    "#                 print(f\"⏱️ Batch load time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "#                 start = time.time()\n",
    "\n",
    "#                 pred = model(slices, point_mask, slice_mask)\n",
    "#                 loss = loss_fn(pred, cd_gt)\n",
    "\n",
    "#                 print(f\"⏱️ Batch forward time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "#                 start = time.time()\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 print(f\"⏱️ Batch backward time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "#                 start = time.time()\n",
    "\n",
    "#                 batch_loss = loss.item() * slices.size(0)\n",
    "#                 total_loss += batch_loss\n",
    "#                 pbar.set_postfix(loss=batch_loss / slices.size(0))\n",
    "\n",
    "#                 print(f\"⏱️ Batch update time: {time.time() - start:.2f} sec\")\n",
    "\n",
    "#             avg_loss = total_loss / len(train_dataloader.dataset)\n",
    "#             epoch_losses.append(avg_loss)\n",
    "#             print(f\"\\n✅ Epoch {epoch}: Avg SmoothL1 Loss = {avg_loss:.7f}\")\n",
    "\n",
    "#             # 🔍 Validation\n",
    "#             print(f\"🔍 Evaluating on validation set:\")\n",
    "#             evaluate_model_on_val(model, val_loader, device, scaler , val_set_size=200)\n",
    "\n",
    "#             print(f\"⏱️ Epoch Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "#             # 💾 Save checkpoint\n",
    "#             ckpt_path = os.path.join(checkpoint_dir, f\"epoch_{epoch:02d}_loss_{avg_loss:.8f}.pt\")\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model': model.state_dict(),\n",
    "#                 'optimizer': optimizer.state_dict(),\n",
    "#                 'epoch_losses': epoch_losses\n",
    "#             }, ckpt_path)\n",
    "#             print(f\"💾 Checkpoint saved: {ckpt_path}\")\n",
    "\n",
    "#             # 🏆 Save best model separately\n",
    "#             if avg_loss < best_loss:\n",
    "#                 best_loss = avg_loss\n",
    "#                 patience_counter = 0\n",
    "#                 torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"best_model.pt\"))\n",
    "#                 print(\"🏆 Best model updated.\")\n",
    "#             else:\n",
    "#                 patience_counter += 1\n",
    "#                 if patience_counter >= early_stopping_patience:\n",
    "#                     print(f\"🛑 Early stopping at epoch {epoch}.\")\n",
    "#                     break\n",
    "\n",
    "#             print(\"-\" * 60)\n",
    "\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"\\n⛔ Interrupted. Saving checkpoint...\")\n",
    "#         torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model': model.state_dict(),\n",
    "#             'optimizer': optimizer.state_dict(),\n",
    "#             'epoch_losses': epoch_losses\n",
    "#         }, os.path.join(checkpoint_dir, f\"interrupted_epoch_{epoch}.pt\"))\n",
    "#         print(\"🧷 Last checkpoint saved.\")\n",
    "\n",
    "#     return model, epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f8fb3",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8541b5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Resuming from checkpoint: ../outputs/checkpoints3\\epoch_99_loss_0.01518300.pt\n",
      "📦 Training on 5398 samples, validating on 200 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100:   1%|          | 12/1350 [00:06<11:32,  1.93batch/s, loss=0.016]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⛔ Interrupted. Saving checkpoint...\n",
      "🧷 Last checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, loss_log = train_model(\n",
    "    resume=True,                        # Set to True if you want to continue from a saved checkpoint\n",
    "    num_epochs=100,\n",
    "    max_cars=None,                       # Use entire dataset\n",
    "    checkpoint_dir=model_checkpoint_dir,\n",
    "    early_stopping_patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92070095",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7648dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_all_checkpoints(checkpoint_dir=\"../outputs/temp_checkpoints\", output_dir=\"../outputs/temp_analysis\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    checkpoint_paths = sorted(glob.glob(os.path.join(checkpoint_dir, \"epoch_*.pt\")))\n",
    "    all_epoch_losses = []\n",
    "    epoch_numbers = []\n",
    "\n",
    "    print(\"\\n📋 Epoch-wise Loss Summary:\")\n",
    "    for ckpt_path in checkpoint_paths:\n",
    "        try:\n",
    "            checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            losses = checkpoint.get(\"epoch_losses\", [])\n",
    "            epoch = checkpoint.get(\"epoch\", None)\n",
    "\n",
    "            if epoch is not None and losses:\n",
    "                epoch_numbers.append(epoch)\n",
    "                last_loss = losses[-1]\n",
    "                all_epoch_losses.append(last_loss)\n",
    "                print(f\"Epoch {epoch:02d} → Loss: {last_loss:.7f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipped {ckpt_path}: {e}\")\n",
    "\n",
    "    if not epoch_numbers:\n",
    "        print(\"❌ No valid checkpoints found.\")\n",
    "        return\n",
    "\n",
    "    # Save compiled loss log\n",
    "    loss_log_path = os.path.join(output_dir, \"compiled_epoch_losses.json\")\n",
    "    with open(loss_log_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"epoch_numbers\": epoch_numbers,\n",
    "            \"epoch_losses\": all_epoch_losses\n",
    "        }, f, indent=2)\n",
    "    print(f\"\\n📄 Saved compiled loss log to: {loss_log_path}\")\n",
    "\n",
    "    # Plot loss curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.lineplot(x=epoch_numbers, y=all_epoch_losses, marker=\"o\")\n",
    "    plt.title(\"📉 Training Loss Curve from Checkpoints\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"SmoothL1 Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"loss_curve_from_ckpts.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot deltas\n",
    "    if len(all_epoch_losses) > 1:\n",
    "        loss_deltas = [all_epoch_losses[i] - all_epoch_losses[i-1] for i in range(1, len(all_epoch_losses))]\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(x=epoch_numbers[1:], y=loss_deltas)\n",
    "        plt.title(\"📊 Δ Loss Between Checkpoints\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Δ Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"loss_deltas_from_ckpts.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"✅ Done analyzing {len(epoch_numbers)} checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdc6bb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Epoch-wise Loss Summary:\n",
      "Epoch 01 → Loss: 0.1635625\n",
      "Epoch 01 → Loss: 0.2464958\n",
      "Epoch 02 → Loss: 0.1042404\n",
      "Epoch 02 → Loss: 0.2155954\n",
      "Epoch 03 → Loss: 0.0837441\n",
      "Epoch 03 → Loss: 0.2063399\n",
      "Epoch 04 → Loss: 0.0860357\n",
      "Epoch 05 → Loss: 0.0824505\n",
      "Epoch 06 → Loss: 0.0715279\n",
      "Epoch 07 → Loss: 0.0671776\n",
      "Epoch 08 → Loss: 0.0637395\n",
      "Epoch 09 → Loss: 0.0586883\n",
      "Epoch 100 → Loss: 0.0171010\n",
      "Epoch 10 → Loss: 0.0572876\n",
      "Epoch 11 → Loss: 0.0540730\n",
      "Epoch 12 → Loss: 0.0495315\n",
      "Epoch 13 → Loss: 0.0461043\n",
      "Epoch 14 → Loss: 0.0459189\n",
      "Epoch 15 → Loss: 0.0455986\n",
      "Epoch 16 → Loss: 0.0438680\n",
      "Epoch 17 → Loss: 0.0424614\n",
      "Epoch 18 → Loss: 0.0422533\n",
      "Epoch 19 → Loss: 0.0410204\n",
      "Epoch 20 → Loss: 0.0398268\n",
      "Epoch 21 → Loss: 0.0387932\n",
      "Epoch 22 → Loss: 0.0394746\n",
      "Epoch 23 → Loss: 0.0366771\n",
      "Epoch 24 → Loss: 0.0372471\n",
      "Epoch 25 → Loss: 0.0361738\n",
      "Epoch 26 → Loss: 0.0356182\n",
      "Epoch 27 → Loss: 0.0354190\n",
      "Epoch 28 → Loss: 0.0344296\n",
      "Epoch 29 → Loss: 0.0343859\n",
      "Epoch 30 → Loss: 0.0342555\n",
      "Epoch 31 → Loss: 0.0338324\n",
      "Epoch 32 → Loss: 0.0330284\n",
      "Epoch 33 → Loss: 0.0329009\n",
      "Epoch 34 → Loss: 0.0323607\n",
      "Epoch 35 → Loss: 0.0316003\n",
      "Epoch 36 → Loss: 0.0311263\n",
      "Epoch 37 → Loss: 0.0321633\n",
      "Epoch 38 → Loss: 0.0339353\n",
      "Epoch 39 → Loss: 0.0292123\n",
      "Epoch 40 → Loss: 0.0292795\n",
      "Epoch 41 → Loss: 0.0308955\n",
      "Epoch 42 → Loss: 0.0300664\n",
      "Epoch 43 → Loss: 0.0289722\n",
      "Epoch 44 → Loss: 0.0290949\n",
      "Epoch 45 → Loss: 0.0279314\n",
      "Epoch 46 → Loss: 0.0280627\n",
      "Epoch 47 → Loss: 0.0427236\n",
      "Epoch 48 → Loss: 0.0295611\n",
      "Epoch 49 → Loss: 0.0304807\n",
      "Epoch 50 → Loss: 0.0278559\n",
      "Epoch 51 → Loss: 0.0277185\n",
      "Epoch 52 → Loss: 0.0274563\n",
      "Epoch 53 → Loss: 0.0260659\n",
      "Epoch 54 → Loss: 0.0260880\n",
      "Epoch 55 → Loss: 0.0274180\n",
      "Epoch 56 → Loss: 0.0249533\n",
      "Epoch 57 → Loss: 0.0255746\n",
      "Epoch 58 → Loss: 0.0250147\n",
      "Epoch 59 → Loss: 0.0241582\n",
      "Epoch 60 → Loss: 0.0240544\n",
      "Epoch 61 → Loss: 0.0240707\n",
      "Epoch 62 → Loss: 0.0238014\n",
      "Epoch 63 → Loss: 0.0231216\n",
      "Epoch 64 → Loss: 0.0226095\n",
      "Epoch 65 → Loss: 0.0236224\n",
      "Epoch 66 → Loss: 0.0221054\n",
      "Epoch 67 → Loss: 0.0224088\n",
      "Epoch 68 → Loss: 0.0216249\n",
      "Epoch 69 → Loss: 0.0218538\n",
      "Epoch 70 → Loss: 0.0211585\n",
      "Epoch 71 → Loss: 0.0204701\n",
      "Epoch 72 → Loss: 0.0202559\n",
      "Epoch 73 → Loss: 0.0195402\n",
      "Epoch 74 → Loss: 0.0198681\n",
      "Epoch 75 → Loss: 0.0211390\n",
      "Epoch 76 → Loss: 0.0197921\n",
      "Epoch 77 → Loss: 0.0196685\n",
      "Epoch 78 → Loss: 0.0199435\n",
      "Epoch 79 → Loss: 0.0176628\n",
      "Epoch 80 → Loss: 0.0178634\n",
      "Epoch 81 → Loss: 0.0177988\n",
      "Epoch 82 → Loss: 0.0173280\n",
      "Epoch 83 → Loss: 0.0169254\n",
      "Epoch 84 → Loss: 0.0170735\n",
      "Epoch 85 → Loss: 0.0176544\n",
      "Epoch 86 → Loss: 0.0166854\n",
      "Epoch 87 → Loss: 0.0161837\n",
      "Epoch 88 → Loss: 0.0150491\n",
      "Epoch 89 → Loss: 0.0160682\n",
      "Epoch 90 → Loss: 0.0158964\n",
      "Epoch 91 → Loss: 0.0149057\n",
      "Epoch 92 → Loss: 0.0144500\n",
      "Epoch 93 → Loss: 0.0154015\n",
      "Epoch 94 → Loss: 0.0143492\n",
      "Epoch 95 → Loss: 0.0141808\n",
      "Epoch 96 → Loss: 0.0138274\n",
      "Epoch 97 → Loss: 0.0156437\n",
      "Epoch 98 → Loss: 0.0147878\n",
      "Epoch 99 → Loss: 0.0151830\n",
      "\n",
      "📄 Saved compiled loss log to: ../outputs/analysis\\compiled_epoch_losses.json\n",
      "✅ Done analyzing 103 checkpoints\n"
     ]
    }
   ],
   "source": [
    "analyze_all_checkpoints(checkpoint_dir=model_checkpoint_dir, output_dir=analysis_output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dda2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(model_checkpoint_dir = model_checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Loads the best model saved in `best_model.pt` using global architecture settings.\n",
    "    \"\"\"\n",
    "    best_model_path = os.path.join(model_checkpoint_dir, \"best_model.pt\")\n",
    "\n",
    "    # Recreate model architecture using global config\n",
    "    pointnet = PointNet2D(input_dim=point_net_input_dim, emb_dim=point_net_embedding_dim)\n",
    "    lstm_encoder = LSTMSliceEncoder(\n",
    "        input_dim=lstm_input_dim,\n",
    "        hidden_dim=lstm_hidden_dim,\n",
    "        num_layers=number_of_layers,\n",
    "        bidirectional=bidirectional\n",
    "    )\n",
    "    regressor = CdRegressor(input_dim=cd_regressor_input_dim)\n",
    "    best_model = CdPredictorNet(pointnet, lstm_encoder, regressor).to(device)\n",
    "\n",
    "    if os.path.exists(best_model_path):\n",
    "        best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        best_model.eval()\n",
    "        print(f\"🏆 Loaded best model from: {best_model_path}\")\n",
    "    else:\n",
    "        print(f\"❌ No best_model.pt found at: {best_model_path}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8c81863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Loaded best model from: ../outputs/checkpoints3\\best_model.pt\n"
     ]
    }
   ],
   "source": [
    "best_model = load_best_model(model_checkpoint_dir)  # New clean instance, separate from training-time model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fd418c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9441\n",
      "🔹 MSE          : 0.000078\n",
      "🔹 RMSE         : 0.008847\n",
      "🔹 MAE          : 0.006645\n",
      "🔹 MAPE         : 2.41%\n",
      "🔹 NMAE         : 3.8950\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Assumes `best_model` is already loaded and set to eval mode\n",
    "assert best_model is not None, \"Best model is not loaded.\"\n",
    "\n",
    "# Load test set IDs\n",
    "id_file = \"../data/subset_dir/test_design_ids.txt\"\n",
    "with open(id_file) as f:\n",
    "    test_ids = [line.strip() for line in f]\n",
    "\n",
    "# Ground-truth Cd map\n",
    "df = pd.read_csv(\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\")\n",
    "cd_map = dict(zip(df[\"Design\"], df[\"Average Cd\"]))\n",
    "\n",
    "# Evaluation setup\n",
    "best_model.eval()\n",
    "preds, trues, ids_used = [], [], []\n",
    "\n",
    "for car_id in test_ids:\n",
    "    npz_path = f\"../outputs/pad_masked_slices/{car_id}_axis-x.npz\"\n",
    "    if not os.path.exists(npz_path):\n",
    "        print(f\"⚠️ Missing file: {car_id}\")\n",
    "        continue\n",
    "\n",
    "    data = np.load(npz_path)\n",
    "    slices = torch.tensor(data[\"slices\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "    point_mask = torch.tensor(data[\"point_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "    slice_mask = torch.tensor(data[\"slice_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cd_scaled = best_model(slices, point_mask, slice_mask).item()\n",
    "        cd_pred = scaler.inverse_transform([[cd_scaled]])[0, 0]\n",
    "\n",
    "    cd_true = cd_map[car_id]\n",
    "    preds.append(cd_pred)\n",
    "    trues.append(cd_true)\n",
    "    ids_used.append(car_id)\n",
    "\n",
    "    # print(f\"🚗 {car_id} → Predicted Cd: {cd_pred:.6f} | True Cd: {cd_true:.6f}\")\n",
    "\n",
    "# Final metrics\n",
    "r2 = r2_score(trues, preds)\n",
    "mse = mean_squared_error(trues, preds)\n",
    "mae = mean_absolute_error(trues, preds)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((np.array(trues) - np.array(preds)) / np.array(trues))) * 100\n",
    "nmae = np.mean(np.abs(np.array(trues) - np.array(preds)) / (np.max(trues) - np.min(trues))) * 100\n",
    "\n",
    "print(\"\\n📊 Evaluation Summary:\")\n",
    "print(f\"🔹 R² Score     : {r2:.4f}\")\n",
    "print(f\"🔹 MSE          : {mse:.6f}\")\n",
    "print(f\"🔹 RMSE         : {rmse:.6f}\")\n",
    "print(f\"🔹 MAE          : {mae:.6f}\")\n",
    "print(f\"🔹 MAPE         : {mape:.2f}%\")\n",
    "print(f\"🔹 NMAE         : {nmae:.4f}\")\n",
    "print(f\"🔹 Cars tested  : {len(preds)} / {len(test_ids)}\")\n",
    "\n",
    "# Save predictions\n",
    "os.makedirs(\"../outputs/eval\", exist_ok=True)\n",
    "out_path = f\"../outputs/eval/cd_predictions_{os.path.basename(id_file).replace('.txt', '')}.csv\"\n",
    "pd.DataFrame({\n",
    "    \"Design ID\": ids_used,\n",
    "    \"Predicted Cd\": preds,\n",
    "    \"True Cd\": trues\n",
    "}).to_csv(out_path, index=False)\n",
    "print(f\"💾 Saved predictions to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "653c4917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9372\n",
      "🔹 MSE          : 0.000081\n",
      "🔹 RMSE         : 0.009022\n",
      "🔹 MAE          : 0.006693\n",
      "🔹 MAPE         : 2.45%\n",
      "🔹 NMAE         : 3.8542\n",
      "🔹 Cars tested  : 1157 / 1157\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_val_design_ids.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Assumes `best_model` is already loaded and set to eval mode\n",
    "assert best_model is not None, \"Best model is not loaded.\"\n",
    "\n",
    "# Load test set IDs\n",
    "id_file = \"../data/subset_dir/val_design_ids.txt\"\n",
    "with open(id_file) as f:\n",
    "    test_ids = [line.strip() for line in f]\n",
    "\n",
    "# Ground-truth Cd map\n",
    "df = pd.read_csv(\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\")\n",
    "cd_map = dict(zip(df[\"Design\"], df[\"Average Cd\"]))\n",
    "\n",
    "# Evaluation setup\n",
    "best_model.eval()\n",
    "preds, trues, ids_used = [], [], []\n",
    "\n",
    "for car_id in test_ids:\n",
    "    npz_path = f\"../outputs/pad_masked_slices/{car_id}_axis-x.npz\"\n",
    "    if not os.path.exists(npz_path):\n",
    "        print(f\"⚠️ Missing file: {car_id}\")\n",
    "        continue\n",
    "\n",
    "    data = np.load(npz_path)\n",
    "    slices = torch.tensor(data[\"slices\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "    point_mask = torch.tensor(data[\"point_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "    slice_mask = torch.tensor(data[\"slice_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cd_scaled = best_model(slices, point_mask, slice_mask).item()\n",
    "        cd_pred = scaler.inverse_transform([[cd_scaled]])[0, 0]\n",
    "\n",
    "    cd_true = cd_map[car_id]\n",
    "    preds.append(cd_pred)\n",
    "    trues.append(cd_true)\n",
    "    ids_used.append(car_id)\n",
    "\n",
    "    # print(f\"🚗 {car_id} → Predicted Cd: {cd_pred:.6f} | True Cd: {cd_true:.6f}\")\n",
    "\n",
    "# Final metrics\n",
    "r2 = r2_score(trues, preds)\n",
    "mse = mean_squared_error(trues, preds)\n",
    "mae = mean_absolute_error(trues, preds)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((np.array(trues) - np.array(preds)) / np.array(trues))) * 100\n",
    "nmae = np.mean(np.abs(np.array(trues) - np.array(preds)) / (np.max(trues) - np.min(trues))) * 100\n",
    "\n",
    "print(\"\\n📊 Evaluation Summary:\")\n",
    "print(f\"🔹 R² Score     : {r2:.4f}\")\n",
    "print(f\"🔹 MSE          : {mse:.6f}\")\n",
    "print(f\"🔹 RMSE         : {rmse:.6f}\")\n",
    "print(f\"🔹 MAE          : {mae:.6f}\")\n",
    "print(f\"🔹 MAPE         : {mape:.2f}%\")\n",
    "print(f\"🔹 NMAE         : {nmae:.4f}\")\n",
    "print(f\"🔹 Cars tested  : {len(preds)} / {len(test_ids)}\")\n",
    "\n",
    "# Save predictions\n",
    "os.makedirs(\"../outputs/eval\", exist_ok=True)\n",
    "out_path = f\"../outputs/eval/cd_predictions_{os.path.basename(id_file).replace('.txt', '')}.csv\"\n",
    "pd.DataFrame({\n",
    "    \"Design ID\": ids_used,\n",
    "    \"Predicted Cd\": preds,\n",
    "    \"True Cd\": trues\n",
    "}).to_csv(out_path, index=False)\n",
    "print(f\"💾 Saved predictions to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e453cb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9751\n",
      "🔹 MSE          : 0.000035\n",
      "🔹 RMSE         : 0.005905\n",
      "🔹 MAE          : 0.004569\n",
      "🔹 MAPE         : 1.64%\n",
      "🔹 NMAE         : 2.5080\n",
      "🔹 Cars tested  : 5398 / 5398\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_train_design_ids.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Assumes `best_model` is already loaded and set to eval mode\n",
    "assert best_model is not None, \"Best model is not loaded.\"\n",
    "\n",
    "# Load test set IDs\n",
    "id_file = \"../data/subset_dir/train_design_ids.txt\"\n",
    "with open(id_file) as f:\n",
    "    test_ids = [line.strip() for line in f]\n",
    "\n",
    "# Ground-truth Cd map\n",
    "df = pd.read_csv(\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\")\n",
    "cd_map = dict(zip(df[\"Design\"], df[\"Average Cd\"]))\n",
    "\n",
    "# Evaluation setup\n",
    "best_model.eval()\n",
    "preds, trues, ids_used = [], [], []\n",
    "\n",
    "for car_id in test_ids:\n",
    "    npz_path = f\"../outputs/pad_masked_slices/{car_id}_axis-x.npz\"\n",
    "    if not os.path.exists(npz_path):\n",
    "        print(f\"⚠️ Missing file: {car_id}\")\n",
    "        continue\n",
    "\n",
    "    data = np.load(npz_path)\n",
    "    slices = torch.tensor(data[\"slices\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "    point_mask = torch.tensor(data[\"point_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "    slice_mask = torch.tensor(data[\"slice_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cd_scaled = best_model(slices, point_mask, slice_mask).item()\n",
    "        cd_pred = scaler.inverse_transform([[cd_scaled]])[0, 0]\n",
    "\n",
    "    cd_true = cd_map[car_id]\n",
    "    preds.append(cd_pred)\n",
    "    trues.append(cd_true)\n",
    "    ids_used.append(car_id)\n",
    "\n",
    "    # print(f\"🚗 {car_id} → Predicted Cd: {cd_pred:.6f} | True Cd: {cd_true:.6f}\")\n",
    "\n",
    "# Final metrics\n",
    "r2 = r2_score(trues, preds)\n",
    "mse = mean_squared_error(trues, preds)\n",
    "mae = mean_absolute_error(trues, preds)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((np.array(trues) - np.array(preds)) / np.array(trues))) * 100\n",
    "nmae = np.mean(np.abs(np.array(trues) - np.array(preds)) / (np.max(trues) - np.min(trues))) * 100\n",
    "\n",
    "print(\"\\n📊 Evaluation Summary:\")\n",
    "print(f\"🔹 R² Score     : {r2:.4f}\")\n",
    "print(f\"🔹 MSE          : {mse:.6f}\")\n",
    "print(f\"🔹 RMSE         : {rmse:.6f}\")\n",
    "print(f\"🔹 MAE          : {mae:.6f}\")\n",
    "print(f\"🔹 MAPE         : {mape:.2f}%\")\n",
    "print(f\"🔹 NMAE         : {nmae:.4f}\")\n",
    "print(f\"🔹 Cars tested  : {len(preds)} / {len(test_ids)}\")\n",
    "\n",
    "# Save predictions\n",
    "os.makedirs(\"../outputs/eval\", exist_ok=True)\n",
    "out_path = f\"../outputs/eval/cd_predictions_{os.path.basename(id_file).replace('.txt', '')}.csv\"\n",
    "pd.DataFrame({\n",
    "    \"Design ID\": ids_used,\n",
    "    \"Predicted Cd\": preds,\n",
    "    \"True Cd\": trues\n",
    "}).to_csv(out_path, index=False)\n",
    "print(f\"💾 Saved predictions to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8d33701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Loaded best model from: ../outputs/checkpoints3\\epoch_95_loss_0.01418081.pt\n"
     ]
    }
   ],
   "source": [
    "def load_best_model(model_checkpoint_dir = model_checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Loads the best model saved in `best_model.pt` using global architecture settings.\n",
    "    \"\"\"\n",
    "    best_model_path = os.path.join(model_checkpoint_dir, \"epoch_95_loss_0.01418081.pt\")\n",
    "\n",
    "    # Recreate model architecture using global config\n",
    "    pointnet = PointNet2D(input_dim=point_net_input_dim, emb_dim=point_net_embedding_dim)\n",
    "    lstm_encoder = LSTMSliceEncoder(\n",
    "        input_dim=lstm_input_dim,\n",
    "        hidden_dim=lstm_hidden_dim,\n",
    "        num_layers=number_of_layers,\n",
    "        bidirectional=bidirectional\n",
    "    )\n",
    "    regressor = CdRegressor(input_dim=cd_regressor_input_dim)\n",
    "    best_model = CdPredictorNet(pointnet, lstm_encoder, regressor).to(device)\n",
    "\n",
    "    if os.path.exists(best_model_path):\n",
    "        best_model.load_state_dict(torch.load(best_model_path, map_location=device)['model'])\n",
    "        best_model.eval()\n",
    "        print(f\"🏆 Loaded best model from: {best_model_path}\")\n",
    "    else:\n",
    "        print(f\"❌ No best_model.pt found at: {best_model_path}\")\n",
    "\n",
    "    return best_model\n",
    "\n",
    "temp_model = load_best_model(model_checkpoint_dir)  # New clean instance, separate from training-time model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d41b9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9458\n",
      "🔹 MSE          : 0.000076\n",
      "🔹 RMSE         : 0.008708\n",
      "🔹 MAE          : 0.006571\n",
      "🔹 MAPE         : 2.39%\n",
      "🔹 NMAE         : 3.8515\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Assumes `temp_model` is already loaded and set to eval mode\n",
    "assert temp_model is not None, \"Best model is not loaded.\"\n",
    "\n",
    "# Load test set IDs\n",
    "id_file = \"../data/subset_dir/test_design_ids.txt\"\n",
    "with open(id_file) as f:\n",
    "    test_ids = [line.strip() for line in f]\n",
    "\n",
    "# Ground-truth Cd map\n",
    "df = pd.read_csv(\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\")\n",
    "cd_map = dict(zip(df[\"Design\"], df[\"Average Cd\"]))\n",
    "\n",
    "# Evaluation setup\n",
    "temp_model.eval()\n",
    "preds, trues, ids_used = [], [], []\n",
    "\n",
    "for car_id in test_ids:\n",
    "    npz_path = f\"../outputs/pad_masked_slices/{car_id}_axis-x.npz\"\n",
    "    if not os.path.exists(npz_path):\n",
    "        print(f\"⚠️ Missing file: {car_id}\")\n",
    "        continue\n",
    "\n",
    "    data = np.load(npz_path)\n",
    "    slices = torch.tensor(data[\"slices\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "    point_mask = torch.tensor(data[\"point_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "    slice_mask = torch.tensor(data[\"slice_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cd_scaled = temp_model(slices, point_mask, slice_mask).item()\n",
    "        cd_pred = scaler.inverse_transform([[cd_scaled]])[0, 0]\n",
    "\n",
    "    cd_true = cd_map[car_id]\n",
    "    preds.append(cd_pred)\n",
    "    trues.append(cd_true)\n",
    "    ids_used.append(car_id)\n",
    "\n",
    "    # print(f\"🚗 {car_id} → Predicted Cd: {cd_pred:.6f} | True Cd: {cd_true:.6f}\")\n",
    "\n",
    "# Final metrics\n",
    "r2 = r2_score(trues, preds)\n",
    "mse = mean_squared_error(trues, preds)\n",
    "mae = mean_absolute_error(trues, preds)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((np.array(trues) - np.array(preds)) / np.array(trues))) * 100\n",
    "nmae = np.mean(np.abs(np.array(trues) - np.array(preds)) / (np.max(trues) - np.min(trues))) * 100\n",
    "\n",
    "print(\"\\n📊 Evaluation Summary:\")\n",
    "print(f\"🔹 R² Score     : {r2:.4f}\")\n",
    "print(f\"🔹 MSE          : {mse:.6f}\")\n",
    "print(f\"🔹 RMSE         : {rmse:.6f}\")\n",
    "print(f\"🔹 MAE          : {mae:.6f}\")\n",
    "print(f\"🔹 MAPE         : {mape:.2f}%\")\n",
    "print(f\"🔹 NMAE         : {nmae:.4f}\")\n",
    "print(f\"🔹 Cars tested  : {len(preds)} / {len(test_ids)}\")\n",
    "\n",
    "# Save predictions\n",
    "os.makedirs(\"../outputs/eval\", exist_ok=True)\n",
    "out_path = f\"../outputs/eval/cd_predictions_{os.path.basename(id_file).replace('.txt', '')}.csv\"\n",
    "pd.DataFrame({\n",
    "    \"Design ID\": ids_used,\n",
    "    \"Predicted Cd\": preds,\n",
    "    \"True Cd\": trues\n",
    "}).to_csv(out_path, index=False)\n",
    "print(f\"💾 Saved predictions to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8cd1a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Loaded best model from: ..\\outputs\\checkpoints3\\epoch_95_loss_0.01418081.pt\n",
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9458\n",
      "🔹 MSE          : 0.000076\n",
      "🔹 RMSE         : 0.008708\n",
      "🔹 MAE          : 0.006571\n",
      "🔹 MAPE         : 2.39%\n",
      "🔹 NMAE         : 3.8515\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n",
      "------------------------------------------------------------\n",
      "🏆 Loaded best model from: ..\\outputs\\checkpoints3\\epoch_93_loss_0.01540153.pt\n",
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9496\n",
      "🔹 MSE          : 0.000070\n",
      "🔹 RMSE         : 0.008395\n",
      "🔹 MAE          : 0.006270\n",
      "🔹 MAPE         : 2.29%\n",
      "🔹 NMAE         : 3.6753\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n",
      "------------------------------------------------------------\n",
      "🏆 Loaded best model from: ..\\outputs\\checkpoints3\\epoch_79_loss_0.01766285.pt\n",
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9507\n",
      "🔹 MSE          : 0.000069\n",
      "🔹 RMSE         : 0.008304\n",
      "🔹 MAE          : 0.006298\n",
      "🔹 MAPE         : 2.30%\n",
      "🔹 NMAE         : 3.6914\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n",
      "------------------------------------------------------------\n",
      "🏆 Loaded best model from: ..\\outputs\\checkpoints3\\epoch_72_loss_0.02025595.pt\n",
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9479\n",
      "🔹 MSE          : 0.000073\n",
      "🔹 RMSE         : 0.008534\n",
      "🔹 MAE          : 0.006547\n",
      "🔹 MAPE         : 2.38%\n",
      "🔹 NMAE         : 3.8374\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n",
      "------------------------------------------------------------\n",
      "🏆 Loaded best model from: ..\\outputs\\checkpoints3\\epoch_71_loss_0.02047008.pt\n",
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9462\n",
      "🔹 MSE          : 0.000075\n",
      "🔹 RMSE         : 0.008678\n",
      "🔹 MAE          : 0.006611\n",
      "🔹 MAPE         : 2.41%\n",
      "🔹 NMAE         : 3.8750\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n",
      "------------------------------------------------------------\n",
      "🏆 Loaded best model from: ..\\outputs\\checkpoints3\\epoch_68_loss_0.02162494.pt\n",
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9525\n",
      "🔹 MSE          : 0.000066\n",
      "🔹 RMSE         : 0.008148\n",
      "🔹 MAE          : 0.006107\n",
      "🔹 MAPE         : 2.23%\n",
      "🔹 NMAE         : 3.5798\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n",
      "------------------------------------------------------------\n",
      "🏆 Loaded best model from: ..\\outputs\\checkpoints3\\epoch_67_loss_0.02240883.pt\n",
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9469\n",
      "🔹 MSE          : 0.000074\n",
      "🔹 RMSE         : 0.008622\n",
      "🔹 MAE          : 0.006482\n",
      "🔹 MAPE         : 2.39%\n",
      "🔹 NMAE         : 3.7993\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n",
      "------------------------------------------------------------\n",
      "🏆 Loaded best model from: ..\\outputs\\checkpoints3\\epoch_66_loss_0.02210539.pt\n",
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9419\n",
      "🔹 MSE          : 0.000081\n",
      "🔹 RMSE         : 0.009015\n",
      "🔹 MAE          : 0.006784\n",
      "🔹 MAPE         : 2.49%\n",
      "🔹 NMAE         : 3.9763\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n",
      "------------------------------------------------------------\n",
      "🏆 Loaded best model from: ..\\outputs\\checkpoints3\\epoch_63_loss_0.02312163.pt\n",
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9483\n",
      "🔹 MSE          : 0.000072\n",
      "🔹 RMSE         : 0.008506\n",
      "🔹 MAE          : 0.006515\n",
      "🔹 MAPE         : 2.39%\n",
      "🔹 NMAE         : 3.8188\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n",
      "------------------------------------------------------------\n",
      "🏆 Loaded best model from: ..\\outputs\\checkpoints3\\epoch_48_loss_0.02956106.pt\n",
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9448\n",
      "🔹 MSE          : 0.000077\n",
      "🔹 RMSE         : 0.008789\n",
      "🔹 MAE          : 0.006563\n",
      "🔹 MAPE         : 2.40%\n",
      "🔹 NMAE         : 3.8468\n",
      "🔹 Cars tested  : 1158 / 1158\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_test_design_ids.csv\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# get oath for the epoch numbers list from the checkpoint dir uisng regular expression\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "# directory containing the saved epoch checkpoints\n",
    "ckpt_dir = Path(\"../outputs/checkpoints3\")\n",
    "\n",
    "# epochs ranked in the previous analysis (best → worst)\n",
    "top_epochs = [72, 68, 67, 93, 71, 79, 48, 66, 95, 63]\n",
    "\n",
    "top_epochs = sorted(top_epochs, reverse=True)  # sort in descending order\n",
    "# grab the first (and usually only) file that matches each epoch pattern\n",
    "path_list = list(\n",
    "    itertools.chain.from_iterable(\n",
    "        ckpt_dir.glob(f\"epoch_{epoch:02d}_loss_*.pt\")   # zero‑padded epoch number\n",
    "        for epoch in top_epochs\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "for path in path_list:\n",
    "\n",
    "    def load_best_model():\n",
    "        \"\"\"\n",
    "        Loads the best model saved in `best_model.pt` using global architecture settings.\n",
    "        \"\"\"\n",
    "        best_model_path = path\n",
    "\n",
    "        # Recreate model architecture using global config\n",
    "        pointnet = PointNet2D(input_dim=point_net_input_dim, emb_dim=point_net_embedding_dim)\n",
    "        lstm_encoder = LSTMSliceEncoder(\n",
    "            input_dim=lstm_input_dim,\n",
    "            hidden_dim=lstm_hidden_dim,\n",
    "            num_layers=number_of_layers,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        regressor = CdRegressor(input_dim=cd_regressor_input_dim)\n",
    "        best_model = CdPredictorNet(pointnet, lstm_encoder, regressor).to(device)\n",
    "\n",
    "        if os.path.exists(best_model_path):\n",
    "            best_model.load_state_dict(torch.load(best_model_path, map_location=device)['model'])\n",
    "            best_model.eval()\n",
    "            print(f\"🏆 Loaded best model from: {best_model_path}\")\n",
    "        else:\n",
    "            print(f\"❌ No best_model.pt found at: {best_model_path}\")\n",
    "\n",
    "        return best_model\n",
    "\n",
    "    temp_model = load_best_model()  # New clean instance, separate from training-time model\n",
    "\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "    # Assumes `temp_model` is already loaded and set to eval mode\n",
    "    assert temp_model is not None, \"Best model is not loaded.\"\n",
    "\n",
    "    # Load test set IDs\n",
    "    id_file = \"../data/subset_dir/test_design_ids.txt\"\n",
    "    with open(id_file) as f:\n",
    "        test_ids = [line.strip() for line in f]\n",
    "\n",
    "    # Ground-truth Cd map\n",
    "    df = pd.read_csv(\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\")\n",
    "    cd_map = dict(zip(df[\"Design\"], df[\"Average Cd\"]))\n",
    "\n",
    "    # Evaluation setup\n",
    "    temp_model.eval()\n",
    "    preds, trues, ids_used = [], [], []\n",
    "\n",
    "    for car_id in test_ids:\n",
    "        npz_path = f\"../outputs/pad_masked_slices/{car_id}_axis-x.npz\"\n",
    "        if not os.path.exists(npz_path):\n",
    "            print(f\"⚠️ Missing file: {car_id}\")\n",
    "            continue\n",
    "\n",
    "        data = np.load(npz_path)\n",
    "        slices = torch.tensor(data[\"slices\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "        point_mask = torch.tensor(data[\"point_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "        slice_mask = torch.tensor(data[\"slice_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cd_scaled = temp_model(slices, point_mask, slice_mask).item()\n",
    "            cd_pred = scaler.inverse_transform([[cd_scaled]])[0, 0]\n",
    "\n",
    "        cd_true = cd_map[car_id]\n",
    "        preds.append(cd_pred)\n",
    "        trues.append(cd_true)\n",
    "        ids_used.append(car_id)\n",
    "\n",
    "        # print(f\"🚗 {car_id} → Predicted Cd: {cd_pred:.6f} | True Cd: {cd_true:.6f}\")\n",
    "\n",
    "    # Final metrics\n",
    "    r2 = r2_score(trues, preds)\n",
    "    mse = mean_squared_error(trues, preds)\n",
    "    mae = mean_absolute_error(trues, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((np.array(trues) - np.array(preds)) / np.array(trues))) * 100\n",
    "    nmae = np.mean(np.abs(np.array(trues) - np.array(preds)) / (np.max(trues) - np.min(trues))) * 100\n",
    "\n",
    "    print(\"\\n📊 Evaluation Summary:\")\n",
    "    print(f\"🔹 R² Score     : {r2:.4f}\")\n",
    "    print(f\"🔹 MSE          : {mse:.6f}\")\n",
    "    print(f\"🔹 RMSE         : {rmse:.6f}\")\n",
    "    print(f\"🔹 MAE          : {mae:.6f}\")\n",
    "    print(f\"🔹 MAPE         : {mape:.2f}%\")\n",
    "    print(f\"🔹 NMAE         : {nmae:.4f}\")\n",
    "    print(f\"🔹 Cars tested  : {len(preds)} / {len(test_ids)}\")\n",
    "\n",
    "    # Save predictions\n",
    "    os.makedirs(\"../outputs/eval\", exist_ok=True)\n",
    "    out_path = f\"../outputs/eval/cd_predictions_{os.path.basename(id_file).replace('.txt', '')}.csv\"\n",
    "    pd.DataFrame({\n",
    "        \"Design ID\": ids_used,\n",
    "        \"Predicted Cd\": preds,\n",
    "        \"True Cd\": trues\n",
    "    }).to_csv(out_path, index=False)\n",
    "    print(f\"💾 Saved predictions to: {out_path}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06182a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Summary:\n",
      "🔹 R² Score     : 0.9724\n",
      "🔹 MSE          : 0.000039\n",
      "🔹 RMSE         : 0.006223\n",
      "🔹 MAE          : 0.004800\n",
      "🔹 MAPE         : 1.74%\n",
      "🔹 NMAE         : 2.6344\n",
      "🔹 Cars tested  : 5398 / 5398\n",
      "💾 Saved predictions to: ../outputs/eval/cd_predictions_train_design_ids.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Assumes `temp_model` is already loaded and set to eval mode\n",
    "assert temp_model is not None, \"Best model is not loaded.\"\n",
    "\n",
    "# Load test set IDs\n",
    "id_file = \"../data/subset_dir/train_design_ids.txt\"\n",
    "with open(id_file) as f:\n",
    "    test_ids = [line.strip() for line in f]\n",
    "\n",
    "# Ground-truth Cd map\n",
    "df = pd.read_csv(\"../data/DrivAerNetPlusPlus_Drag_8k_cleaned.csv\")\n",
    "cd_map = dict(zip(df[\"Design\"], df[\"Average Cd\"]))\n",
    "\n",
    "# Evaluation setup\n",
    "temp_model.eval()\n",
    "preds, trues, ids_used = [], [], []\n",
    "\n",
    "for car_id in test_ids:\n",
    "    npz_path = f\"../outputs/pad_masked_slices/{car_id}_axis-x.npz\"\n",
    "    if not os.path.exists(npz_path):\n",
    "        print(f\"⚠️ Missing file: {car_id}\")\n",
    "        continue\n",
    "\n",
    "    data = np.load(npz_path)\n",
    "    slices = torch.tensor(data[\"slices\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "    point_mask = torch.tensor(data[\"point_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "    slice_mask = torch.tensor(data[\"slice_mask\"], dtype=torch.float32).unsqueeze(0).to(device, non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cd_scaled = temp_model(slices, point_mask, slice_mask).item()\n",
    "        cd_pred = scaler.inverse_transform([[cd_scaled]])[0, 0]\n",
    "\n",
    "    cd_true = cd_map[car_id]\n",
    "    preds.append(cd_pred)\n",
    "    trues.append(cd_true)\n",
    "    ids_used.append(car_id)\n",
    "\n",
    "    # print(f\"🚗 {car_id} → Predicted Cd: {cd_pred:.6f} | True Cd: {cd_true:.6f}\")\n",
    "\n",
    "# Final metrics\n",
    "r2 = r2_score(trues, preds)\n",
    "mse = mean_squared_error(trues, preds)\n",
    "mae = mean_absolute_error(trues, preds)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((np.array(trues) - np.array(preds)) / np.array(trues))) * 100\n",
    "nmae = np.mean(np.abs(np.array(trues) - np.array(preds)) / (np.max(trues) - np.min(trues))) * 100\n",
    "\n",
    "print(\"\\n📊 Evaluation Summary:\")\n",
    "print(f\"🔹 R² Score     : {r2:.4f}\")\n",
    "print(f\"🔹 MSE          : {mse:.6f}\")\n",
    "print(f\"🔹 RMSE         : {rmse:.6f}\")\n",
    "print(f\"🔹 MAE          : {mae:.6f}\")\n",
    "print(f\"🔹 MAPE         : {mape:.2f}%\")\n",
    "print(f\"🔹 NMAE         : {nmae:.4f}\")\n",
    "print(f\"🔹 Cars tested  : {len(preds)} / {len(test_ids)}\")\n",
    "\n",
    "# Save predictions\n",
    "os.makedirs(\"../outputs/eval\", exist_ok=True)\n",
    "out_path = f\"../outputs/eval/cd_predictions_{os.path.basename(id_file).replace('.txt', '')}.csv\"\n",
    "pd.DataFrame({\n",
    "    \"Design ID\": ids_used,\n",
    "    \"Predicted Cd\": preds,\n",
    "    \"True Cd\": trues\n",
    "}).to_csv(out_path, index=False)\n",
    "print(f\"💾 Saved predictions to: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu-copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
